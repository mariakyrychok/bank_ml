{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Customers have been leaving Beta Bank every month. Although the numbers are not large, the trend is noticeable. Bank marketers have determined that retaining existing clients is more cost-effective than acquiring new ones. The task is to predict whether a customer will leave the bank in the near future. Historical data on customer behavior and contract terminations with the bank are provided. The goal is to build a model with the highest possible F1-score. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Open and study the file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.781643Z",
     "start_time": "2024-06-11T12:31:51.779468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0     2.0       0.00              1          1               1   \n1     1.0   83807.86              1          0               1   \n2     8.0  159660.80              3          1               0   \n3     1.0       0.00              2          0               0   \n4     2.0  125510.82              1          1               1   \n\n   EstimatedSalary  Exited  \n0        101348.88       1  \n1        112542.58       0  \n2        113931.57       1  \n3         93826.63       0  \n4         79084.10       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2.0</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1.0</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8.0</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2.0</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/churn.csv')\n",
    "display(df.head())\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.907557Z",
     "start_time": "2024-06-11T12:31:51.780789Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the dataset, there are 10,000 objects and 14 features (`Exited` being the target feature). Each object in the dataset represents information about customer behavior and contract terminations with the bank. The following is known:\n",
    "\n",
    "- *RowNumber* — index of the row in the data\n",
    "- *CustomerId* — unique customer identifier\n",
    "- *Surname* — surname\n",
    "- *CreditScore* — credit score\n",
    "- *Geography* — country of residence\n",
    "- *Gender* — gender\n",
    "- *Age* — age\n",
    "- *Tenure* — number of years the person has been a customer of the bank\n",
    "- *Balance* — account balance\n",
    "- *NumOfProducts* — number of bank products used by the customer\n",
    "- *HasCrCard* — presence of a credit card\n",
    "- *IsActiveMember* — customer's activity\n",
    "- *EstimatedSalary* — estimated salary\n",
    "- *Exited* — fact of customer churn\n",
    "\n",
    "There are 9% missing values in the `Tenure` column. There is a violation of style in the names of all columns. To proceed further, the data issues need to be addressed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Header Style\n",
    "Let's rename the columns:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "df.columns = ['row_number', 'customer_id', 'surname', 'credit_score', 'geography', 'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card', 'is_active_member', 'estimated_salary', 'excited']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.909868Z",
     "start_time": "2024-06-11T12:31:51.806551Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Most likely, the blanks in the `tenure` column indicate that this user has recently become a bank customer. Therefore, let's fill these missing values with zero:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df['tenure'] = df['tenure'].fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.910511Z",
     "start_time": "2024-06-11T12:31:51.808737Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useless Data\n",
    "\n",
    "In our table, there are data that have no potential connection with the outcome of the work. These data are not only useful for the model but can also be harmful. Therefore, we will get rid of columns such as `row_number`, `customer_id`, `surname`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "df = df.drop(['row_number', 'customer_id', 'surname'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.911003Z",
     "start_time": "2024-06-11T12:31:51.811701Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding (OHE)\n",
    "\n",
    "Our data contains categorical features `geography` and `gender`. To avoid errors during model training, we will transform categorical features into numerical ones using the technique of One-Hot Encoding (OHE), and to avoid falling into the dummy variable trap, we'll use the `pd.get_dummies()` function with the `drop_first` argument."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.911499Z",
     "start_time": "2024-06-11T12:31:51.813382Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's convert all column names to lowercase:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.913178Z",
     "start_time": "2024-06-11T12:31:51.819719Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Splitting\n",
    "\n",
    "To perform the classification task, we first need to split the data into three sets: training, validation, and testing. We'll split the original data in a 3:1:1 ratio. Initially, we'll use the `train_test_split` method to separate the training set from the data. Then, using the same method, we'll split the remaining data into validation and testing sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.4, random_state=12345)\n",
    "df_test, df_valid = train_test_split(df_test, test_size=0.5, random_state=12345)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.933217Z",
     "start_time": "2024-06-11T12:31:51.822364Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each dataset, let's designate the target feature (`target`) and other features (`features`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_features_and_target(data):\n",
    "    return data.drop('excited', axis=1), data['excited']\n",
    "\n",
    "features_train, target_train = get_features_and_target(df_train)\n",
    "\n",
    "features_valid, target_valid = get_features_and_target(df_valid)\n",
    "\n",
    "features_test, target_test = get_features_and_target(df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.933866Z",
     "start_time": "2024-06-11T12:31:51.827178Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling\n",
    "\n",
    "To avoid the trap where the algorithm might consider one feature more important than another, features are scaled—brought to the same scale. Let's standardize the features using `StandardScaler`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "      credit_score       age    tenure   balance  num_of_products  \\\n7041     -2.226392 -0.088482 -0.825373 -1.233163         0.830152   \n5709     -0.087120  0.006422  1.426375 -1.233163        -0.891560   \n7117     -0.917905 -0.752805  0.139662  0.722307        -0.891560   \n7775     -0.253277  0.101325  1.748053 -1.233163         0.830152   \n8735      0.785204 -0.847708  1.748053  0.615625        -0.891560   \n\n      has_cr_card  is_active_member  estimated_salary  geography_germany  \\\n7041            1                 0          0.647083              False   \n5709            1                 0         -1.658410              False   \n7117            1                 1         -1.369334              False   \n7775            1                 0          0.075086              False   \n8735            0                 1         -1.070919              False   \n\n      geography_spain  gender_male  \n7041            False         True  \n5709            False        False  \n7117             True         True  \n7775             True         True  \n8735            False         True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>credit_score</th>\n      <th>age</th>\n      <th>tenure</th>\n      <th>balance</th>\n      <th>num_of_products</th>\n      <th>has_cr_card</th>\n      <th>is_active_member</th>\n      <th>estimated_salary</th>\n      <th>geography_germany</th>\n      <th>geography_spain</th>\n      <th>gender_male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7041</th>\n      <td>-2.226392</td>\n      <td>-0.088482</td>\n      <td>-0.825373</td>\n      <td>-1.233163</td>\n      <td>0.830152</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.647083</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5709</th>\n      <td>-0.087120</td>\n      <td>0.006422</td>\n      <td>1.426375</td>\n      <td>-1.233163</td>\n      <td>-0.891560</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1.658410</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7117</th>\n      <td>-0.917905</td>\n      <td>-0.752805</td>\n      <td>0.139662</td>\n      <td>0.722307</td>\n      <td>-0.891560</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.369334</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7775</th>\n      <td>-0.253277</td>\n      <td>0.101325</td>\n      <td>1.748053</td>\n      <td>-1.233163</td>\n      <td>0.830152</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.075086</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8735</th>\n      <td>0.785204</td>\n      <td>-0.847708</td>\n      <td>1.748053</td>\n      <td>0.615625</td>\n      <td>-0.891560</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.070919</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric = ['credit_score', 'age', 'tenure', 'balance', 'num_of_products', 'estimated_salary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "features_valid.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.938097Z",
     "start_time": "2024-06-11T12:31:51.830703Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Balance\n",
    "\n",
    "Let's check if our classes are balanced in the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "excited\n0    0.7963\n1    0.2037\nName: proportion, dtype: float64"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['excited'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:51.945879Z",
     "start_time": "2024-06-11T12:31:51.840828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our task, there is a significant class imbalance (4:1), which will negatively affect model training. To address this imbalance, we can use techniques such as class weighting, upsampling, and downsampling. However, we'll follow the sequence of tasks in the project, and for now, we'll train the model on imbalanced data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prototype Solution Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our target feature is categorical, which means we are dealing with a classification task, specifically binary classification, since there are only two categories (\"customer churned\" — `exited = 1`, \"customer stayed\" — `exited = 0`).\n",
    "\n",
    "For solving this task, the following models will be suitable:\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "We will sequentially train these three models and then evaluate them.\n",
    "\n",
    "First, we will train the decision tree model. To achieve the highest level of prediction quality, we will try different tree depths ranging from 1 to 30 during the training process.\n",
    "\n",
    "Since there is class imbalance in the data, we will use the F1-score (the harmonic mean of precision and recall) as the metric for evaluating all models, instead of accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best decision tree model on the validation set: 0.5378. \n",
      "Tree depth: 9\n"
     ]
    }
   ],
   "source": [
    "random_state = 12345\n",
    "\n",
    "def decision_tree(features_train, target_train, features_valid, target_valid, class_weight=None):\n",
    "    best_model = None\n",
    "    best_result = 0\n",
    "    best_depth = 1\n",
    "\n",
    "    for depth in range(1, 30, 1):\n",
    "        model = DecisionTreeClassifier(random_state=random_state, max_depth=depth, class_weight=class_weight)\n",
    "        model.fit(features_train, target_train)\n",
    "        predicted_valid = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, predicted_valid)\n",
    "        if result > best_result:\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "            best_result = result\n",
    "    return best_result, best_depth, best_model\n",
    "\n",
    "decision_tree_result, decision_tree_depth, _ = decision_tree(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best decision tree model on the validation set: {decision_tree_result:.4}. ',\n",
    "      f'Tree depth: {decision_tree_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:31:52.610941Z",
     "start_time": "2024-06-11T12:31:51.845076Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model with the best F1-score (0.5378) on the validation set turned out to be the model with a decision tree depth of 9. This is not a very good result, but it's important to remember that we didn't account for class imbalance.\n",
    "\n",
    "Let's try training a random forest model. To find the best model, we will tune another hyperparameter – the number of trees (n_estimators) from 10 to 100 with a step of 10."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best random forest model on the validation set: 0.5531\n",
      "Number of trees: 50\n",
      "Tree depth: 18\n"
     ]
    }
   ],
   "source": [
    "def random_forest(features_train, target_train, features_valid, target_valid, class_weight=None):\n",
    "    best_model = None\n",
    "    best_result = 0\n",
    "    best_est = 10\n",
    "    best_depth = 1\n",
    "\n",
    "    for est in range(10, 100, 10):\n",
    "        for depth in range(1, 30, 1):\n",
    "            model = RandomForestClassifier(\n",
    "                random_state=random_state,\n",
    "                n_estimators=est,\n",
    "                max_depth=depth,\n",
    "                class_weight=class_weight\n",
    "            )\n",
    "            model.fit(features_train, target_train)\n",
    "            predicted_valid = model.predict(features_valid)\n",
    "            result = f1_score(target_valid, predicted_valid)\n",
    "            if result > best_result:\n",
    "                best_model = model\n",
    "                best_result = result\n",
    "                best_est = est\n",
    "                best_depth = depth\n",
    "    return best_result, best_est, best_depth, best_model\n",
    "\n",
    "forest_result, forest_est, forest_depth, _ = random_forest(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best random forest model on the validation set: {forest_result:.4}',\n",
    "      f'Number of trees: {forest_est}',\n",
    "      f'Tree depth: {forest_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:32:42.894793Z",
     "start_time": "2024-06-11T12:31:52.408165Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model with the best F1-score (0.5531) on the validation set turned out to be the model with 50 trees and a depth of 18. As we can see, the F1-score of the random forest model is slightly higher than that of the decision tree model, but this value is still insufficient for an acceptable result. Additionally, a drawback of the random forest model is its execution speed: the more trees, the slower the model works.\n",
    "\n",
    "Let's see what F1-score the logistic regression model will achieve on the imbalanced classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the logistic regression model on the validation set: 0.2743\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(features_train, target_train, features_valid, target_valid, class_weight=None):\n",
    "    model = LogisticRegression(random_state=random_state, solver='liblinear', class_weight=class_weight)\n",
    "    model.fit(features_train, target_train)\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    result = f1_score(target_valid,predicted_valid)\n",
    "    return result, model\n",
    "\n",
    "logistic_regression_result, _ = logistic_regression(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the logistic regression model on the validation set: {logistic_regression_result:.4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:32:42.928204Z",
     "start_time": "2024-06-11T12:32:42.827946Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Among the three models, the logistic regression model has the lowest F1-score - 0.2743.\n",
    "\n",
    "However, let's move on to class balancing. Perhaps on balanced data, this model will show a better result."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solving Imbalance\n",
    "\n",
    "As we decided earlier, to address the imbalance, we can use techniques such as class weighting, upsampling, and downsampling. Let's start with the first method - class weighting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Weighting\n",
    "\n",
    "By specifying *class_weight='balanced'* in the parameters of our algorithms, the algorithm will calculate how many times class \"0\" occurs more frequently than class \"1\". Let's denote this number as N, and the new classes will look like this:\n",
    "- Class \"0\" weight = 1.0\n",
    "- Class \"1\" weight = N"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the logistic regression model on the validation set: 0.4797\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_result, _ = logistic_regression(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid,\n",
    "    'balanced'\n",
    ")\n",
    "print(f'F1-score of the logistic regression model on the validation set: {logistic_regression_result:.4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:32:42.928678Z",
     "start_time": "2024-06-11T12:32:42.842030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best random forest model on the validation set: 0.6197\n",
      "Number of trees: 70\n",
      "Tree depth: 9\n"
     ]
    }
   ],
   "source": [
    "forest_result, forest_est, forest_depth, _ = random_forest(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid,\n",
    "    'balanced'\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best random forest model on the validation set: {forest_result:.4}',\n",
    "      f'Number of trees: {forest_est}',\n",
    "      f'Tree depth: {forest_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:33:35.956534Z",
     "start_time": "2024-06-11T12:32:42.858127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best decision tree model on the validation set: 0.5809. \n",
      "Tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "decision_tree_result, decision_tree_depth, _ = decision_tree(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid,\n",
    "    'balanced'\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best decision tree model on the validation set: {decision_tree_result:.4}. ',\n",
    "      f'Tree depth: {decision_tree_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:33:36.552515Z",
     "start_time": "2024-06-11T12:33:35.949801Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "Class weighting helped us achieve a decent result. Among the three algorithms, RandomForest stands out, with a model using 70 trees and a tree depth of 9 giving us an F1-score of 0.6197."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upsampling\n",
    "\n",
    "We will repeat the rare class several times (in our case, 4 times)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:33:36.559751Z",
     "start_time": "2024-06-11T12:33:36.554721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the logistic regression model on the validation set: 0.4779\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_result, _ = logistic_regression(\n",
    "    features_upsampled,\n",
    "    target_upsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "print(f'F1-score of the logistic regression model on the validation set: {logistic_regression_result:.4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:33:36.948618Z",
     "start_time": "2024-06-11T12:33:36.560515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best decision tree model on the validation set: 0.5809. \n",
      "Tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "decision_tree_result, decision_tree_depth, _ = decision_tree(\n",
    "    features_upsampled,\n",
    "    target_upsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best decision tree model on the validation set: {decision_tree_result:.4}. ',\n",
    "      f'Tree depth: {decision_tree_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:33:37.399660Z",
     "start_time": "2024-06-11T12:33:36.843407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best random forest model on the validation set: 0.6206\n",
      "Number of trees: 30\n",
      "Tree depth: 11\n"
     ]
    }
   ],
   "source": [
    "forest_result, forest_est, forest_depth, _ = random_forest(\n",
    "    features_upsampled,\n",
    "    target_upsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best random forest model on the validation set: {forest_result:.4}',\n",
    "      f'Number of trees: {forest_est}',\n",
    "      f'Tree depth: {forest_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:34:56.794673Z",
     "start_time": "2024-06-11T12:33:37.399383Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "By duplicating instances of the minority class 4 times, we balanced the classes. This helped us achieve an F1-score of 0.6206 (Random Forest model with 30 trees and depth of 11)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downsampling\n",
    "\n",
    "Instead of repeating the rare class (1), we'll remove a portion of class 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=random_state)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=random_state)] + [target_ones])\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=random_state)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:34:57.031728Z",
     "start_time": "2024-06-11T12:34:56.652919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the logistic regression model on the validation set: 0.4863\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_result, _ = logistic_regression(\n",
    "    features_downsampled,\n",
    "    target_downsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "print(f'F1-score of the logistic regression model on the validation set: {logistic_regression_result:.4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:34:57.090044Z",
     "start_time": "2024-06-11T12:34:56.927742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best decision tree model on the validation set: 0.6074. \n",
      "Tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "decision_tree_result, decision_tree_depth, _ = decision_tree(\n",
    "    features_downsampled,\n",
    "    target_downsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best decision tree model on the validation set: {decision_tree_result:.4}. ',\n",
    "      f'Tree depth: {decision_tree_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:34:57.250213Z",
     "start_time": "2024-06-11T12:34:56.928580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the best random forest model on the validation set: 0.5906\n",
      "Number of trees: 10\n",
      "Tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "forest_result, forest_est, forest_depth, _ = random_forest(\n",
    "    features_downsampled,\n",
    "    target_downsampled,\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")\n",
    "\n",
    "print(f'F1-score of the best random forest model on the validation set: {forest_result:.4}',\n",
    "      f'Number of trees: {forest_est}',\n",
    "      f'Tree depth: {forest_depth}', sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:35:21.760581Z",
     "start_time": "2024-06-11T12:34:57.093705Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "When downsampling the dataset, two models achieved an F1-score above 0.59:\n",
    "- Decision tree model with tree depth 5 - 0.6074\n",
    "- Random forest model with 10 trees and depth 5 - 0.5906."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Testing\n",
    "\n",
    "Let's select the four models from the previous task that achieved an F1-score above 0.59 on the validation set and compare their performance on the test set.\n",
    "\n",
    "Additionally, we'll measure the AUC-ROC value on the test set and compare it with the F1-score.\n",
    "\n",
    "After completing this task, we'll draw conclusions and choose the most suitable model for our problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "                             name  f1_score_on_valid  f1_score_on_test  \\\n0  Random Forest: class weighting             0.6197            0.6224   \n1       Random Forest: upsampling             0.6206            0.6121   \n2     Decision Tree: downsampling             0.6074            0.5931   \n3     Random Forest: downsampling             0.5906            0.5890   \n\n   auc_roc_on_test    speed  \n0           0.8537  0.03183  \n1           0.8434  0.00578  \n2           0.8229  0.00051  \n3           0.8379  0.00137  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>f1_score_on_valid</th>\n      <th>f1_score_on_test</th>\n      <th>auc_roc_on_test</th>\n      <th>speed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest: class weighting</td>\n      <td>0.6197</td>\n      <td>0.6224</td>\n      <td>0.8537</td>\n      <td>0.03183</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random Forest: upsampling</td>\n      <td>0.6206</td>\n      <td>0.6121</td>\n      <td>0.8434</td>\n      <td>0.00578</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Decision Tree: downsampling</td>\n      <td>0.6074</td>\n      <td>0.5931</td>\n      <td>0.8229</td>\n      <td>0.00051</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Random Forest: downsampling</td>\n      <td>0.5906</td>\n      <td>0.5890</td>\n      <td>0.8379</td>\n      <td>0.00137</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    {\n",
    "        'name': 'Random Forest: class weighting',\n",
    "        'model': RandomForestClassifier(\n",
    "            random_state=random_state, n_estimators=70, max_depth=9, class_weight='balanced'),\n",
    "        'features': features_train,\n",
    "        'target': target_train,\n",
    "        'f1_score_on_valid': 0.6197\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest: upsampling',\n",
    "        'model': RandomForestClassifier(random_state=random_state, n_estimators=30, max_depth=11),\n",
    "        'features': features_upsampled,\n",
    "        'target': target_upsampled,\n",
    "        'f1_score_on_valid': 0.6206\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree: downsampling',\n",
    "        'model': DecisionTreeClassifier(random_state=random_state, max_depth=5),\n",
    "        'features': features_downsampled,\n",
    "        'target': target_downsampled,\n",
    "        'f1_score_on_valid': 0.6074\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest: downsampling',\n",
    "        'model': RandomForestClassifier(random_state=random_state, n_estimators=10, max_depth=5),\n",
    "        'features': features_downsampled,\n",
    "        'target': target_downsampled,\n",
    "        'f1_score_on_valid': 0.5906\n",
    "    },\n",
    "]\n",
    "\n",
    "for model_obj in models:\n",
    "    model = model_obj['model']\n",
    "    model.fit(model_obj['features'], model_obj['target'])\n",
    "\n",
    "    #speed\n",
    "    start = time.time()\n",
    "    predicted_test = model.predict(features_test)\n",
    "    end = time.time()\n",
    "    speed = end - start\n",
    "\n",
    "    #f1_score\n",
    "    f1 =  f1_score(target_test, predicted_test)\n",
    "\n",
    "    #auc_roc\n",
    "    probabilities_test = model.predict_proba(features_test)\n",
    "    probabilities_one_test = probabilities_test[:, 1]\n",
    "    auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "    model_obj['f1_score_on_test'] = round(f1, 4)\n",
    "    model_obj['speed'] = round(speed, 5)\n",
    "    model_obj['auc_roc_on_test'] = round(auc_roc, 4)\n",
    "\n",
    "display(pd.DataFrame(models, columns=['name', 'f1_score_on_valid', 'f1_score_on_test', 'auc_roc_on_test', 'speed']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-06-11T12:35:22.401654Z",
     "start_time": "2024-06-11T12:35:21.752825Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "With F1-score values ranging from 0.58 to 0.62, the AUC-ROC values look plausible, ranging from 0.82 to 0.85. The selected models are not perfect, but they provide accuracy better than a random model.\n",
    "\n",
    "On the test set, the best F1-score was achieved by the **Random Forest** model with 70 trees and depth 9, using class weighting for balancing – 0.6224. However, this model showed the worst prediction speed. If speed is an important parameter for the client, then the best model in this case is the **Decision Tree** (depth 5, balanced using downsampling): this model has lower accuracy but is several times faster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
